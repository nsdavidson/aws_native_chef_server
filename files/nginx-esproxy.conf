#user opscode opscode;
worker_processes 8;

events {
  worker_connections 10240;
}

http {

  include /usr/local/openresty/nginx/conf/esproxy_upstream.conf;

  log_format opscode '$http_x_forwarded_for - $remote_user [$time_local]  '
                    '"$request" $status "$request_time" $body_bytes_sent '
                    '"$http_referer" "$http_user_agent" "$upstream_addr" "$upstream_status" "$upstream_response_time" "$http_x_chef_version" "$http_x_ops_sign" "$http_x_ops_userid" "$http_x_ops_timestamp" "$http_x_ops_content_hash" $request_length';

  server_names_hash_bucket_size 128;

  sendfile on;
  tcp_nopush on;
  tcp_nodelay on;

  keepalive_timeout 65;

  gzip on;
  gzip_http_version 1.0;
  gzip_comp_level 2;
  gzip_proxied any;
  gzip_types text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript application/json;

  lua_package_path "/var/opt/opscode/nginx-esproxy/etc/scripts/?.lua;$prefix/?.lua;;";

  client_body_temp_path "/var/opt/opscode/nginx-esproxy/tmp/client_body";

  # This is for the IAM credentials cache in scripts/aws_creds.lua
  lua_shared_dict iam_creds 128k;

  # We add the server definition here even though the file is called
  # upstreams.conf because the location it is included in the nginx.conf file is
  # the correct location for server definitions.

  server {
    # Listen on localhost because we don't want everyone accessing 
    # elasticsearch, and AWS creds
    listen 127.0.0.1:9200;

    client_max_body_size 100M;

    access_log /var/log/opscode/nginx-esproxy/elasticsearch.access.log opscode;
    error_log  /var/log/opscode/nginx-esproxy/elasticsearch.error.log;

    # This lets us access the AWS metadata from lua
    location "/_awsapi" {
      proxy_pass http://169.254.169.254/;
    }

    location "/" {
      rewrite_by_lua_file '/var/opt/opscode/nginx-esproxy/etc/scripts/es_auth.lua';
      proxy_pass https://elasticsearch;
    }
  }

  # the "real" ELB upstream, a workaround to upstreams only being resolved once, which 
  # is no bueno for ELBs
#   server {
#     listen 127.0.0.1:8001;

#     access_log /var/log/opscode/nginx-esproxy/tarpit.access.log opscode;
#     error_log  /var/log/opscode/nginx-esproxy/tarpit.error.log;

#     <% @nameservers.each do |ns| %>
#     resolver <%= ns %> valid=60s;
#     <% end %>

#     set $upstream_endpoint http://<%= @elb %>:8000;
#     location / {
#       proxy_pass $upstream_endpoint;
#     }
#   }
}